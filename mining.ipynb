{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# PRAIA MINING \n",
    "\n",
    "Hoje vamos aprender a minerar dados com a conversa do grupo!\n",
    "\n",
    "O primeiro experimento vai ser a geração da base, a exploração da mesma e por fim a geração de uma wordcloud.\n",
    "\n",
    "Se você ainda não exportou a conversa do grupo, faça isso indo nas opções da conversa e clicando em mais e depois em exportar conversa.\n",
    "\n",
    "No nosso caso a conversa está salva na mesma pasta que o código, com o nome de 'conversa.txt'. Se você salvar em um lugar diferente, basta modificar abaixo.\n",
    "O código a seguir será o responsável por abrir o arquivo e salvar seu conteúdo na variável wpp_test.\n",
    "\n",
    "Mas antes de rodar qualquer uma das células, **ATENÇÃO!**\n",
    "\n",
    "Pelo terminal, dentro do repositório, instale os requisitos usando o comando `pip install -r requirements.txt`. Na verade, talvez você não precise desse passo caso esteja rodando o notebook, porque o Jupyter vai rodar o código no próprio servidor. Mas caso precise, já sabe o que fazer.\n",
    "\n",
    "E lembre-se de rodar as células em ordem, caso contrário, o código pode se comportar de maneira inesperada."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o arquivo\n",
    "wpp_text = open('conversa.txt').read()\n",
    "\n",
    "# Exibe os primeiros 1000 caracteres para nos dar uma ideia do conteúdo da base. \n",
    "# Também vale a pena dar uma olhada rápida no arquivo para ter uma ideia do que estaremos lidando.\n",
    "wpp_text[:1000]"
   ]
  },
  {
   "source": [
    "Agora que já extraímos o texto do arquivo da conversa precisamos transformar esses dados em algo mais fácil de manipular do que uma sequência de caracteres!\n",
    "\n",
    "De cara já podemos ver que toda linha possui uma estrutura: <data\\> <horário> - <quem enviou a mensagem\\>: <mensagem\\>. Agora, o que queremos é transformar a conversa em uma tabela, onde cada uma dessas informações presente numa linha vai ficar em uma coluna. \n",
    "\n",
    "Nessa parte, como todas as outras, é importante explorar a base, usando por exemplo a função `split()` em diferentes linhas para encontrar uma forma de separar as informações de uma maneira que funcione para todas as linhas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o primeiro split quebra o texto em uma lista de linhas, usando como divisor o carácter de quebra de linha ('\\n') \n",
    "# o segundo split quebra a linha 250 em uma lista, usando o carácter espaço (' ')\n",
    "wpp_text.split('\\n')[250].split(' ')"
   ]
  },
  {
   "source": [
    "Seguindo a ideia de explorar a base, aqui podemos ter uma ideia que quantas linhas temos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wpp_text.split('\\n'))"
   ]
  },
  {
   "source": [
    "Depois de explorar a base, precisamos escolher uma forma de transforma-la em uma tabela, tabela essa que será um DataFrame, uma das estrutura de dados implementadas pelo pacote Pandas.\n",
    "\n",
    "Outra estrutura implementada pelo Pandas é a Series, que também utilizaremos aqui, já que ela possue um método para gerar um DataFrame a partir de expressões regulares, o `Series.str.extract()`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# wpp_text.split('\\n') quebra o texto em uma lista de linhas, \n",
    "# e pd.Series() transforma essa lista em uma Series, que é uma estrutura mais parecida com um vetor\n",
    "wpp_series = pd.Series(wpp_text.split('\\n'))\n",
    "\n",
    "# essa linha imprime o resultado\n",
    "wpp_series"
   ]
  },
  {
   "source": [
    "A parte da geração do dataframe a partir de uma expressão regular segue logo abaixo mas talvez você olhe a primeira linha e se pergunte: que p***a tá acontecendo aqui?\n",
    "\n",
    "Relaxe, eu explico.\n",
    "\n",
    "O método `Series.str.extract()` transforma os grupos de captura de uma expressão regular nas colunas do nosso DataFrame.\n",
    "\n",
    "E os grupos de captura são as partes da expresão regular que estão envolvidas em parênteses! \n",
    "\n",
    "Se lique no que cada um deles vai capturar:\n",
    "\n",
    " - (\\d{2}\\/\\d{2}\\/\\d{4})\n",
    " \n",
    " Essa é a captura da data. \n",
    " \n",
    " **\\d{2}** captura uma sequência de dois dígitos, se houver 4 no lugar do 2, 4 dígitos são capturados.\n",
    " \n",
    " **\\/** captura o caráctere \\ (barra).\n",
    " \n",
    " Combinando esses elementos, conseguimos capturar uma sequência de 2 dígitos, seguida de \\, seguida de outra de 2 dígitos, seguida de barra, seguida de uma sequência de 4 **( ͡° ͜ʖ ͡°)** \n",
    " \n",
    " Ou seja, uma data.\n",
    "\n",
    " - Depois desse primeiro grupo, há um espaço, indicando que ante as strings capturadas deve haver uma espaço\n",
    "\n",
    " - (\\d{2}:\\d{2})\n",
    "\n",
    " A diferença desse grupo para o primeiro é que aqui deve haver : entre duas sequências de 2 dígitos, ou seja, hora.\n",
    "\n",
    " - Novamente um espaço, seguido de - e de outro espaço\n",
    "\n",
    " - (.*?:)\n",
    "\n",
    " Captura de nome ou número de contato\n",
    "\n",
    " . (ponto) irá casar com qualquer caractere, \n",
    " \n",
    " \\+ (mais) determina que deve haver um ou mais caracteres, \n",
    " \n",
    " ? (interrogação) diz que a captura não deve ser gulosa, ou seja, assim que a expressão for satisfeita, a busca deve parar, \n",
    " \n",
    " : (dois pontos) deve ser o último da string capturada pelo grupo.\n",
    "\n",
    " - (.*)\n",
    "\n",
    " Captura da mensagem, que vai ser qualquer caractere que venha depois do : (dois pontos)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wpp_series.str.extract(r'(\\d{2}\\/\\d{2}\\/\\d{4}) (\\d{2}:\\d{2}) - (.+?:) (.*)')\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "Às vezes nossa base pode vir com algumas inconscistências, como dados faltando. No caso da nossa, algums linhas vieram com NaN, valores que podemos descartar com o método `DataFrame.dropna()`\n",
    "\n",
    "Ao final da saída que será imprimida, é possivel ver que a quantidade de linhas da tabela diminuiu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "Esse passo será útil quando formos manipular as colunas: dar nomes a elas."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['date', 'time', 'sender', 'message']\n",
    "print(df)"
   ]
  },
  {
   "source": [
    "Com as colunas nomeadas podemos facilmente, por exemplo, selecionar a coluna sender (que guarda o nome do contato que enviou a mensagem) e usar nela o método unique(), nos retorna uma lista de valores únicos."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sender.unique())"
   ]
  },
  {
   "source": [
    "Caso sua base venha com algum contato sem nome (somente com o número), ou deseje renomear o sender por algum outro motivo, faça como foi feito aqui."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Nome e númerosão são somente representativos, por motivos de privacidade\n",
    "df.sender[df.sender == '+55 71 9999-9999:'] = 'Fulano da Silva Júnior'"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "E se quisermos saber quantos mensagens cada contato enviou, qual foi a mensagem mais enviada por cada um, etc etc?\n",
    "\n",
    "Podemos usar o método `groupby()` passando nossa coluna 'sender' como parêmetro, o que vai agrupar as linhas com base nos valores dessa coluna, e em cima desse resultado usar o método `describe()`, que vai resumir os dados em números, como média, número de mensagens, mensagem mais enviada e algumas outras informações."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sender').describe()"
   ]
  },
  {
   "source": [
    "E agora que temos o número de mensagens enviadas podemos usar o método `sort_values()` para ordenar os contatos ('sender') em sentido decrescente (ascending=False) usando a coluna 'count' (by='count') como critério de ordenação."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sender').describe().date.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "source": [
    "Mas ainda não acabamos e tratar a base! ~~E quando você chegar ao final desse notebook vai ver que ainda haverão coisas a serem exploradas~~\n",
    "\n",
    "Nossa base tem mensagens que eram arquivos de mídia, mas esses arquvos não estão na nossa base e saber que eles foram enviados não vai nos servir pra nada, por enquanto. \n",
    "\n",
    "O que fazemos então? Geramos uma cópia do DataFrame e retiramos as linhas onde as mensagens são <Arquivo de mídia oculto> com o método drop, que vai receber como parêmetro uma lista com os índices que serão excluídos ~~tô com preguiça de explicar melhor agora~~\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_media = df.copy()\n",
    "\n",
    "df_no_media = df_no_media.drop(df_no_media[df_no_media.message == '<Arquivo de mídia oculto>'].index).reset_index()\n",
    "df_no_media\n"
   ]
  },
  {
   "source": [
    "Agora finalmente vamos gerar a wordcloud! \n",
    "Usamos a classe WordCloud do pacote wordcloud e passamos como parêmetro uma lista de stopwords (palavras muito comuns que nada agragarão à nossa visualização)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Gera a wordcloud\n",
    "wc = WordCloud(max_words=80, background_color=\"white\").generate(' '.join(df_no_media.message))\n",
    "\n",
    "# Plota a wordcloud como imagem\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui pegamos uma lista de stopwords presente no pacote nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repetimos o processo anterior, mas dessa vez com as stopwords do nltk\n",
    "wc = WordCloud(stopwords=stopwords, max_words=80, background_color=\"white\").generate(' '.join(df_no_media.message))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "A partir dessa imagem conseguimos ver que ainda estão presentes alguma palavram que não são interessantes. Não tem problema, podemos acrescentá-las à nossa lista de stopwords"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Stopwords identificadas a olho e adicinadas na mão grande\n",
    "new_stopwords = stopwords + ['aí', 'né', 'pra', 'pq', 'tá', 'faz', 'pq', 'se', 'vc', 'https', 'www', 'tbm', 'voce', 'você', 'ia', 'já', 'tava']\n",
    "\n",
    "# Stopwords presentes em outra lista que encontrei na internet\n",
    "other_list = open('stopwords-pt.txt').read().split('\\n')\n",
    "\n",
    "# Stopwords finais, juntando todas as outras\n",
    "final_stopwords = new_stopwords + other_list\n",
    "\n",
    "# Geramos e plotamos a WordCloud novamente\n",
    "wc = WordCloud(stopwords=final_stopwords, max_words=100, background_color=\"white\").generate(' '.join(df_no_media.message))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Por último salvamos a imagem gerada num arquivo png."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.to_file('praia_wordcloud.png')"
   ]
  },
  {
   "source": [
    "Mas e agora, o que mais pode ser feito?\n",
    "\n",
    "Ao olhar melhor o arquivo das conversas, vi que mensagens com quebra de linha têm seu conteúdo desprezado pelo método `Series.str.extract()`, por causa do jeito que a expressão regular foi escrita. Como peegar essas mensagens quebradas?\n",
    "\n",
    "Outras informações que podem ser interessantes são as mudanças de nome do grupo, mudanças de número, mudança de descrição e menções, que também não são extraídas.\n",
    "\n",
    "Além disso, ficam aqui mais umas sugestões\n",
    "- Gerar uma nuvem de palavras para cada membro do grupo\n",
    "- Unificar algumas palavras, como kkkk com diferentes quantidades de k, ou considerar as palavras como case insensitive\n",
    "\n",
    "E é isso, tá aí a zorra do código :)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
